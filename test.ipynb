{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/XPretrain-colab/blob/main/test.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "\n",
    "!pip install datasets yt_dlp scenedetect[opencv]\n",
    "\n",
    "import datasets, json\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "!mkdir /content/videos\n",
    "\n",
    "def download_video(id):\n",
    "  ydl_opts = {'overwrites':True, 'format':'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4', 'outtmpl':f'/content/videos/{id}.mp4'}\n",
    "  with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(id)\n",
    "    return f\"/content/videos/{id}.mp4\"\n",
    "\n",
    "dataset = datasets.load_dataset(\"camenduru/hdvila_test\", split='train[:1%]')\n",
    "# dataset = datasets.load_dataset(\"camenduru/microsoft-XPretrain\", split='train[:1%]')\n",
    "first_2_data = list(dataset)[:2]\n",
    "\n",
    "for dump in first_2_data:\n",
    "  json_string = json.dumps(dump)\n",
    "  row = json.loads(json_string)\n",
    "  video_id = row['video_id']\n",
    "  print(video_id)\n",
    "  try:\n",
    "    download_video(video_id)\n",
    "    video_path = f\"/content/videos/{video_id}.mp4\"\n",
    "    # !scenedetect -i {video_path} -o /content/adaptive --drop-short-scenes detect-adaptive split-video\n",
    "    !scenedetect -i {video_path} -o /content/content --drop-short-scenes detect-content split-video\n",
    "    # !scenedetect -i {video_path} -o /content/threshold --drop-short-scenes detect-threshold split-video\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "\n",
    "!pip install yt_dlp scenedetect[opencv]\n",
    "\n",
    "import json\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "!mkdir /content/videos\n",
    "\n",
    "def download_video(id):\n",
    "  ydl_opts = {'overwrites':True, 'format':'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4', 'outtmpl':f'/content/videos/{id}.mp4'}\n",
    "  with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(f\"https://www.imdb.com/video/{id}\")\n",
    "    return f\"/content/videos/{id}.mp4\"\n",
    "\n",
    "import requests, json\n",
    "url_popular= \"https://camenduru-imdb.hf.space/api/chart/popular\"\n",
    "response_popular = requests.get(url_popular)\n",
    "if response_popular.status_code == 200:\n",
    "  json_data_popular = response_popular.json()\n",
    "  json_string_popular = json.dumps(json_data_popular)\n",
    "  movies = json.loads(json_string_popular)['data']\n",
    "  # movies = movies[2:] # skip first 2\n",
    "  for movie in movies:\n",
    "    url_titles = f\"https://camenduru-imdb.hf.space/api/titles/{movie['titleId']}\"\n",
    "    response_titles = requests.get(url_titles)\n",
    "    if response_titles.status_code == 200:\n",
    "      json_data_titles = response_titles.json()\n",
    "      json_string_titles = json.dumps(json_data_titles)\n",
    "      videos = json.loads(json_string_titles)['data']['videos']\n",
    "      my_array = []\n",
    "      my_dict = {}\n",
    "      for video in videos:\n",
    "        if 'Trailer' in video['duration']:\n",
    "          minutes, seconds = video['duration'].split()[1].split(\":\")\n",
    "          minutes = int(minutes) * 60\n",
    "          seconds = int(seconds)\n",
    "          total_seconds = minutes + seconds\n",
    "          my_array.append(total_seconds)\n",
    "          my_dict[total_seconds] = video['id']\n",
    "      download_video_id = my_dict.get(max(my_array))\n",
    "      print(download_video_id)\n",
    "      download_video(download_video_id)\n",
    "      video_path = f\"/content/videos/{download_video_id}.mp4\"\n",
    "      !scenedetect -i {video_path} -o /content/content --drop-short-scenes detect-content split-video save-images -n 1\n",
    "    else:\n",
    "      print(\"Error getting JSON data:\", response.status_code)\n",
    "\n",
    "else:\n",
    "  print(\"Error getting JSON data:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_top_10_largest_files(folder_path):\n",
    "    file_sizes = []\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            file_sizes.append((file_path, file_size))\n",
    "\n",
    "    sorted_files = sorted(file_sizes, key=lambda x: x[1], reverse=True)\n",
    "    top_10_files = sorted_files[:10]\n",
    "    return top_10_files\n",
    "\n",
    "folder_path = \"/content/content\"\n",
    "top_10_files = find_top_10_largest_files(folder_path)\n",
    "for file_path, file_size in top_10_files:\n",
    "    print(f\"File: {file_path} | Size: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "\n",
    "!git clone -b dev https://github.com/camenduru/Video-BLIP2-Preprocessor\n",
    "%cd /content/Video-BLIP2-Preprocessor\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python preprocess.py --video_directory /content/content --config_name \"My Videos\" --config_save_name \"my_videos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt -y install -qq aria2\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/test_blip2/resolve/main/test_content.zip -d /content/models -o test_content.zip\n",
    "!unzip /content/models/test_content.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/scheduler/scheduler_config.json -d /content/models/model_scope_diffusers/scheduler -o scheduler_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/text_encoder/config.json -d /content/models/model_scope_diffusers/text_encoder -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/resolve/main/text_encoder/pytorch_model.bin -d /content/models/model_scope_diffusers/text_encoder -o pytorch_model.bin\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/tokenizer/merges.txt -d /content/models/model_scope_diffusers/tokenizer -o merges.txt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/tokenizer/special_tokens_map.json -d /content/models/model_scope_diffusers/tokenizer -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/tokenizer/tokenizer_config.json -d /content/models/model_scope_diffusers/tokenizer -o tokenizer_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/tokenizer/vocab.json -d /content/models/model_scope_diffusers/tokenizer -o vocab.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/unet/config.json -d /content/models/model_scope_diffusers/unet -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/resolve/main/unet/diffusion_pytorch_model.bin -d /content/models/model_scope_diffusers/unet -o diffusion_pytorch_model.bin\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/vae/config.json -d /content/models/model_scope_diffusers/vae -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/resolve/main/vae/diffusion_pytorch_model.bin -d /content/models/model_scope_diffusers/vae -o diffusion_pytorch_model.bin\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/raw/main/model_index.json -d /content/models/model_scope_diffusers -o model_index.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/camenduru/Text-To-Video-Finetuning\n",
    "%cd Text-To-Video-Finetuning\n",
    "!git clone https://huggingface.co/damo-vilab/text-to-video-ms-1.7b /content/models/model_scope_diffusers/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py --config /content/Text-To-Video-Finetuning/configs/v2/train_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path: \"/content/models/model_scope_diffusers/\" #https://huggingface.co/damo-vilab/text-to-video-ms-1.7b/tree/main\n",
    "output_dir: \"/content/outputs\"\n",
    "dataset_types: \n",
    "  - 'json'\n",
    "offset_noise_strength: 0.1\n",
    "use_offset_noise: False\n",
    "extend_dataset: False\n",
    "cache_latents: True\n",
    "cached_latent_dir: null\n",
    "train_text_encoder: False\n",
    "use_unet_lora: True\n",
    "use_text_lora: True\n",
    "unet_lora_modules:\n",
    "  - \"ResnetBlock2D\"\n",
    "text_encoder_lora_modules:\n",
    "  - \"CLIPEncoderLayer\"\n",
    "lora_rank: 16\n",
    "train_data:\n",
    "  width: 512      \n",
    "  height: 512\n",
    "  use_bucketing: True\n",
    "  sample_start_idx: 1\n",
    "  fps: 24 \n",
    "  frame_step: 5\n",
    "  n_sample_frames: 8\n",
    "  json_path: '/content/Text-To-Video-Finetuning/test_blip2/test.json'\n",
    "  single_video_path: \"path/to/single/video.mp4\"\n",
    "  single_video_prompt: \"\"\n",
    "  fallback_prompt: ''\n",
    "  path: \"path/to/folder/of/videos/\"\n",
    "  image_dir: 'path/to/image/directory'\n",
    "  single_img_prompt: \"\"\n",
    "validation_data:\n",
    "  prompt: \"duck\"\n",
    "  sample_preview: True\n",
    "  num_frames: 16\n",
    "  width: 512\n",
    "  height: 512\n",
    "  num_inference_steps: 25\n",
    "  guidance_scale: 9\n",
    "learning_rate: 5e-6\n",
    "adam_weight_decay: 1e-2\n",
    "extra_unet_params: null\n",
    "  #learning_rate: 1e-5\n",
    "  #adam_weight_decay: 1e-4\n",
    "extra_text_encoder_params: null\n",
    "  #learning_rate: 5e-6\n",
    "  #adam_weight_decay: 0.2\n",
    "train_batch_size: 1\n",
    "max_train_steps: 10000\n",
    "checkpointing_steps: 2500\n",
    "validation_steps: 100\n",
    "trainable_modules:\n",
    "  - \"attn1\"\n",
    "  - \"attn2\"\n",
    "  - 'temp_conv'\n",
    "trainable_text_modules:\n",
    "  - \"all\"\n",
    "seed: 64\n",
    "mixed_precision: \"fp16\"\n",
    "use_8bit_adam: False\n",
    "gradient_checkpointing: True\n",
    "text_encoder_gradient_checkpointing: False\n",
    "enable_xformers_memory_efficient_attention: False\n",
    "enable_torch_2_attn: True\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
