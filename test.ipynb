{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/XPretrain-colab/blob/main/test.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets yt_dlp scenedetect[opencv]\n",
    "\n",
    "import datasets, json\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "!mkdir /content/videos\n",
    "\n",
    "def download_video(id):\n",
    "  ydl_opts = {'overwrites':True, 'format':'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4', 'outtmpl':f'/content/videos/{id}.mp4'}\n",
    "  with YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download(id)\n",
    "    return f\"/content/videos/{id}.mp4\"\n",
    "\n",
    "dataset = datasets.load_dataset(\"camenduru/hdvila_test\", split='train[:1%]')\n",
    "# dataset = datasets.load_dataset(\"camenduru/microsoft-XPretrain\", split='train[:1%]')\n",
    "first_2_data = list(dataset)[:2]\n",
    "\n",
    "for dump in first_2_data:\n",
    "  json_string = json.dumps(dump)\n",
    "  row = json.loads(json_string)\n",
    "  video_id = row['video_id']\n",
    "  print(video_id)\n",
    "  try:\n",
    "    download_video(video_id)\n",
    "    clips = row[\"clip\"]\n",
    "    for clip in clips:\n",
    "      video_path = f\"/content/videos/{video_id}.mp4\"\n",
    "      # !scenedetect -i {video_path} -o /content/adaptive --drop-short-scenes detect-adaptive split-video\n",
    "      !scenedetect -i {video_path} -o /content/content --drop-short-scenes detect-content split-video\n",
    "      # !scenedetect -i {video_path} -o /content/threshold --drop-short-scenes detect-threshold split-video\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def find_top_10_largest_files(folder_path):\n",
    "    file_sizes = []\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            file_sizes.append((file_path, file_size))\n",
    "\n",
    "    sorted_files = sorted(file_sizes, key=lambda x: x[1], reverse=True)\n",
    "    top_10_files = sorted_files[:10]\n",
    "    return top_10_files\n",
    "\n",
    "folder_path = \"/content/content\"\n",
    "top_10_files = find_top_10_largest_files(folder_path)\n",
    "for file_path, file_size in top_10_files:\n",
    "    print(f\"File: {file_path} | Size: {file_size} bytes\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
